{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "뉴스크롤링_scrapycode.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP1cFTSdO2y39m5csBoeR1s",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hugekyung/TIL/blob/master/Crawl/%EB%89%B4%EC%8A%A4%ED%81%AC%EB%A1%A4%EB%A7%81_scrapycode.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zG2kRVYMwrGq"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "import scrapy\n",
        "import re\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "# from naver_news.items import NaverNewsItem\n",
        "# 연합뉴스 기준\n",
        "# scrapy crawl news\n",
        "\n",
        "\n",
        "class CrawlYeonhabNews(scrapy.Spider):\n",
        "    name = 'news'\n",
        "    allowed_domains = ['news.naver.com']\n",
        "    url_format = \"https://news.naver.com/main/list.nhn?mode=LPOD&mid=sec&oid=001&listType=summary&date={}&page={}\"\n",
        "\n",
        "    def __init__(self):\n",
        "        start = input(\"시작 날짜를 입력하세요(ex.2020-09-30): \")\n",
        "        end = input(\"마지막 날짜를 입력하세요(ex.2020-09-30): \")\n",
        "        startdate =  datetime.strptime(start, \"%Y-%m-%d\")\n",
        "        enddate =  datetime.strptime(end, \"%Y-%m-%d\")\n",
        "\n",
        "        self.start_urls= []\n",
        "        for cur_date in pd.date_range(startdate, enddate):\n",
        "            self.start_urls.append(self.url_format.format(cur_date.strftime(\"%Y%m%d\")))\n",
        "\n",
        "    def start_requests(self):\n",
        "        for start_url in self.start_urls:\n",
        "            # timer = round((time.time() - self.starttime) / 60)\n",
        "            # percentage = round(self.request_count / len(self.inner_id_ls) * 100, 2)\n",
        "            # print(\"{}분 경과, {}개 중 {}개 request 완료({}%)\".format(timer, self.request_count, len(self.inner_id_ls), percentage))\n",
        "            yield scrapy.Request(url=start_url, callback=self.parse)\n",
        "\n",
        "    def parse(self, response):\n",
        "        for item in response.css(\"ul.type06_headline\"):\n",
        "            # upload_date = item.css(\"span.date\").get()\n",
        "            url = item.css(\"a::attr(href)\").get()\n",
        "            yield scrapy.Request(url, callback=self.parse_detail)\n",
        "\n",
        "        for item2 in response.css(\"ul.type06\"):\n",
        "            url2 = item2.css(\"a::attr(href)\").get()\n",
        "            yield scrapy.Request(url2, callback=self.parse_detail)\n",
        "\n",
        "        # 마지막 페이지 개수 어떻게 세지?\n",
        "        next_page = response.css('a.paging::attr(href)').get()\n",
        "        if next_page is not None:\n",
        "            next_page = response.urljoin(next_page)\n",
        "            yield scrapy.Request(next_page, callback=self.parse)\n",
        "    \n",
        "    def parse_detail(self, response):\n",
        "        upload_date = response.css(\"div.sponsor span.t11::text\").get()\n",
        "        title = response.css(\"h3#articleTitle::text\").get()\n",
        "        # media = response.css(\"div.press_logo img::attr(title)\").get()\n",
        "        text = ''.join(response.css(\"div#articleBodyContents::text\").getall()).replace(\"\\n\",\"\").strip()\n",
        "        if response.css(\"div.media_end_head_autosummary _auto_summary_wrapper a.media_end_head_autosummary_button _toggle_btn nclicks(sum_summary)::text\") == \"요약봇\":\n",
        "            # summary_bot = \n",
        "            # 요약봇 결과가 서버에 다녀오는 형식이라 클릭해야 데이터가 온다 어쩌지? \n",
        "            # css말고 json형식으로 가져오는건가?\n",
        "\n",
        "        yield {\n",
        "                    \"upload_date\": upload_date,\n",
        "                    \"title\": title,\n",
        "                    \"text\": text,\n",
        "                    \"summary_bot\": summary_bot\n",
        "                }"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}