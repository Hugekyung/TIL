{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "뉴스크롤링_scrapycode.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMKQvTcbAMDI2IHfg9mf+jV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hugekyung/TIL/blob/master/Crawl/%EB%89%B4%EC%8A%A4%ED%81%AC%EB%A1%A4%EB%A7%81_scrapycode.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zG2kRVYMwrGq"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "import scrapy\n",
        "import re\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "import time\n",
        "import json\n",
        "import requests\n",
        "# 네이버 뉴스 '사회' 카테고리\n",
        "# 기사 수 TOP3 연합뉴스, 뉴시스, 뉴스1\n",
        "# 연합뉴스 https://news.naver.com/main/list.nhn?mode=LPOD&mid=sec&oid=001&date=20201023\n",
        "# 뉴시스 https://news.naver.com/main/list.nhn?mode=LPOD&mid=sec&oid=003&date=20201023\n",
        "# 뉴스1 https://news.naver.com/main/list.nhn?mode=LPOD&mid=sec&oid=421&date=20201023\n",
        "\n",
        "class CrawlNews(scrapy.Spider):\n",
        "    name = \"news\"\n",
        "    allowed_domains = [\"news.naver.com\", \"tts.news.naver.com\"]\n",
        "    url_format = \"https://news.naver.com/main/list.nhn?mode=LPOD&mid=sec&oid=001&date={}\" # 연합뉴스\n",
        "\n",
        "\n",
        "    def __init__(self):\n",
        "        start = input(\"시작 날짜를 입력하세요(ex.2020-09-30): \")\n",
        "        end = input(\"마지막 날짜를 입력하세요(ex.2020-09-30): \")\n",
        "        startdate =  datetime.strptime(start, \"%Y-%m-%d\")\n",
        "        enddate =  datetime.strptime(end, \"%Y-%m-%d\")\n",
        "\n",
        "        self.start_urls= []\n",
        "        for cur_date in pd.date_range(startdate, enddate):\n",
        "            self.start_urls.append(self.url_format.format(cur_date.strftime(\"%Y%m%d\")))\n",
        "\n",
        "\n",
        "    def start_requests(self):\n",
        "        for start_url in self.start_urls: # 가져올 페이지를 리스트에 저장\n",
        "            yield scrapy.Request(url=start_url, callback=self.parse, meta={\"start_url\" : start_url, \"page_num\": 1})\n",
        "            time.sleep(5)\n",
        "\n",
        "\n",
        "    def parse(self, response):\n",
        "        page_num = response.meta[\"page_num\"]\n",
        "        if int(response.css(\"div.paging strong::text\").get()) != page_num:\n",
        "            return # 함수 종료\n",
        "        else:\n",
        "            for item in response.css(\"ul.type06_headline li\"):\n",
        "                url = item.css(\"a::attr(href)\").get()\n",
        "                yield scrapy.Request(url, callback=self.parse_detail)\n",
        "\n",
        "            for item2 in response.css(\"ul.type06 li\"):\n",
        "                url2 = item2.css(\"a::attr(href)\").get()\n",
        "                yield scrapy.Request(url2, callback=self.parse_detail)\n",
        "                \n",
        "        # 다음 페이지 url\n",
        "        page_num += 1\n",
        "        start_url = response.meta[\"start_url\"]\n",
        "        next_page = start_url + \"&page={}\".format(page_num)\n",
        "        print(\"다음 페이지============\", next_page)\n",
        "        yield scrapy.Request(next_page, callback=self.parse, meta={\"start_url\" : start_url, \"page_num\" : page_num})\n",
        "    \n",
        "\n",
        "    def parse_detail(self, response):\n",
        "        try:\n",
        "            if response.css(\"div.media_end_head_autosummary._auto_summary_wrapper > a::text\").get() == '요약봇':\n",
        "                upload_date = response.css(\"div.sponsor span.t11::text\").get()\n",
        "                media = response.css(\"div.press_logo img::attr(title)\").get()\n",
        "                title = response.css(\"h3#articleTitle::text\").get()\n",
        "                text = ''.join(response.css(\"div#articleBodyContents::text\").getall()).replace(\"\\n\",\"\").strip()\n",
        "\n",
        "                # text 전처리(연합뉴스용 - 다른 언론사 기사 추가 필요)\n",
        "                temp_text = text\n",
        "                temp_text = re.sub('\\(([A-Za-z가-힣]*=[가-힣]*\\))','',str(temp_text)) # (서울=연합뉴스) or (AP=연합뉴스) 형식 제거\n",
        "                temp_text = re.sub('.*[가-힣] 기자 =','',temp_text) # 기자이름 제거\n",
        "                temp_text = re.sub('.*[가-힣] 특파원 =','',temp_text)\n",
        "                temp_text = re.sub('.*[가-힣] 기자=','',temp_text)\n",
        "                temp_text = re.sub('    ','',temp_text)\n",
        "                temp_text = re.sub('\\'','',temp_text)\n",
        "                temp_text = re.sub('.*[가-힣]기자','',temp_text) # 기자이름 제거\n",
        "                temp_text = re.sub('\\\\xa0','',temp_text) # .*\\\\xa0.* 포함된 거 제거\n",
        "                temp_text = re.sub('[A-Za-z]+@yna.co.kr.*','',temp_text) # 이메일 제거\n",
        "                temp_text = re.sub('[A-Za-z]+@yonhapnews.co.kr.*','',temp_text) # 이메일 제거\n",
        "                temp_text = re.sub('([0-9a-zA-Z]{1,100}\\.[0-9a-zA-Z]{0,100}%?)','',temp_text) # 00.00형태 삭제\n",
        "                # temp_text = re.sub('[가-힣]{1}\\.','',temp_text) # .제거\n",
        "                temp_text.strip()\n",
        "                text = temp_text\n",
        "\n",
        "                # 뉴스 원문 url\n",
        "                # https://news.naver.com/main/read.nhn?mode=LSD&mid=sec&sid1=102&oid=032&aid=0003037750\n",
        "                # oid: 언론사 구분 번호, aid: 해당 기사 할당 번호\n",
        "                \n",
        "                news_url = response.css(\"div.article_btns_right > a::attr(href)\").get()\n",
        "                oid = re.search('oid=([0-9]..)', news_url).group()\n",
        "                oid = re.search('[0-9]+', oid).group()\n",
        "                aid = re.search('(\\d+)(?!.*\\d)', news_url).group()\n",
        "                # print('aid는 이거~~~~~~', aid)\n",
        "                summary_url = \"https://tts.news.naver.com/article/{}/{}/summary?callback=window\".format(oid, aid)\n",
        "                yield scrapy.Request(summary_url, callback=self.parse_summary_bot, meta={\"summary_url\" : summary_url, \"upload_date\" : upload_date, \"media\" : media, \"title\" : title, \"text\" : text})\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "\n",
        "    def parse_summary_bot(self, response):\n",
        "        print('요약봇 response 잘 받앗음!!!!!!', response)\n",
        "        upload_date = response.meta[\"upload_date\"]\n",
        "        media = response.meta[\"media\"]\n",
        "        title = response.meta[\"title\"]\n",
        "        text = response.meta[\"text\"]\n",
        "        \n",
        "        try:\n",
        "            window = response.text # json형식이 아니다\n",
        "            window = re.sub('.+(\"summary\":)', '', window)\n",
        "            sent_lst = window.split('<br/><br/>')\n",
        "            summary_text = [re.sub('[\\\\\\\\\"});]+', '', x) for x in sent_lst]\n",
        "            # print(summary_text)\n",
        "        except:\n",
        "            print('error!')\n",
        "        \n",
        "        # 전체 결과 출력\n",
        "        yield {\n",
        "                    \"upload_date\": upload_date,\n",
        "                    \"media\": media,\n",
        "                    \"title\": title,\n",
        "                    \"text\": text,\n",
        "                    \"summary_text\": summary_text,\n",
        "            }"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}