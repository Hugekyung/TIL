{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# df = pd.read_csv(r'C:\\Users\\haech\\Insta_profiling\\spider\\insta_crawling\\gw_region.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3719"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.unique(df['inner_id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gw_region을 기반으로 id : next_page 사전화\n",
    "df_dropna = df.dropna(subset=['next_page']) # next_page칼럼에 결측치가 있으면 해당 행을 삭제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_ls = list(map(str, df_dropna['inner_id']))\n",
    "next_ls = list(map(str, df_dropna['next_page']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 순서를 유지하며 unique값 뽑아내기\n",
    "# np.unique(id_ls)\n",
    "index = np.unique(id_ls, return_index=True)[1]\n",
    "uni_id_ls = [id_ls[idx] for idx in sorted(index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = np.unique(next_ls, return_index=True)[1]\n",
    "uni_next_ls = [next_ls[idx] for idx in sorted(index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 딕셔너리화\n",
    "id2npage = {}\n",
    "for i, id in enumerate(uni_id_ls):\n",
    "    for idx, n_page in enumerate(uni_next_ls):\n",
    "        if i == idx:\n",
    "            id2npage[id] = n_page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 한글 텍스트인 계정만 추출하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "df = pd.read_csv(r'C:\\Users\\haech\\Insta_profiling\\spider\\insta_crawling\\gw_region.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3719"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_lst = list(set(df['insta_id']))\n",
    "len(id_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def del_foreign(df):\n",
    "    id_lst = list(set(df['insta_id']))\n",
    "    for user_id in id_lst:\n",
    "        count = 0\n",
    "        for y in list(df[df['insta_id']==user_id]['content']):\n",
    "            if type(y) != str:\n",
    "                pass\n",
    "            elif re.search('[가-힣]', y):\n",
    "                count += 1\n",
    "        if count == 0:\n",
    "            idx = df[df['insta_id']==user_id].index\n",
    "            df.drop(idx, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = del_foreign(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2749"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_lst = list(set(new_df['insta_id']))\n",
    "len(id_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.to_csv('gw_hangul.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gw_hangul = pd.read_csv(r'C:\\Users\\haech\\Insta_profiling\\spider\\insta_crawling\\gw_hangul.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "hangul_ls = list(map(str, list(np.unique(gw_hangul['inner_id']))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2749"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hangul_ls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 순서를 유지하며 unique값 뽑아내고, 딕셔너리 생성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dic(df):\n",
    "    inner_id_ls = list(map(str, list(np.unique(df['inner_id']))))\n",
    "    df_dropnana = df.dropna(subset=['next_page'])\n",
    "    id_ls = list(map(str, df_dropna['inner_id']))\n",
    "    next_ls = list(map(str, df_dropna['next_page']))\n",
    "\n",
    "    # 순서를 유지하며 unique값 뽑아내기\n",
    "    index = np.unique(id_ls, return_index=True)[1]\n",
    "    uni_id_ls = [id_ls[idx] for idx in sorted(index)]\n",
    "\n",
    "    index = np.unique(next_ls, return_index=True)[1]\n",
    "    uni_next_ls = [next_ls[idx] for idx in sorted(index)]\n",
    "\n",
    "    # 딕셔너리 생성\n",
    "    for i, id in enumerate(uni_id_ls):\n",
    "        for idx, n_page in enumerate(uni_next_ls):\n",
    "            if i == idx:\n",
    "                id2npage[id] = n_page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_dic(gw_hangul)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2541"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(id2npage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 팔로워 1000명 이상 id 제외 / 좋아요 1000개 이상 게시물 있는 id 제외"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "over1000_df = pd.read_csv(r'C:\\Users\\haech\\Insta_profiling\\spider\\insta_crawling\\gw_over1000.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def del_over1000(df, over1000_df):\n",
    "    # 팔로워 1000명 이상 id 제외\n",
    "    for over1000_id in list(over1000_df['data']):\n",
    "        idx = df[df['insta_id']==over1000_id].index\n",
    "        df.drop(idx, inplace=True)\n",
    "    \n",
    "    # 좋아요 1000개 이상 게시물 있는 id 제외\n",
    "    df['like_count'] = df['like_count'].apply(int)\n",
    "    id_lst = list(set(df['insta_id']))\n",
    "    for user_id in id_lst:\n",
    "        count = 0\n",
    "        for y in list(df[df['insta_id']==user_id]['like_count']):\n",
    "            if y > 1000:\n",
    "                count += 1\n",
    "        if count > 1:\n",
    "            idx = df[df['insta_id']==user_id].index\n",
    "            df.drop(idx, inplace=True)\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_new_df = del_over1000(new_df, over1000_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2376"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_lst = list(set(new_new_df['insta_id']))\n",
    "len(id_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_new_df.to_csv('gw_delete.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "gw_delete = pd.read_csv(r'C:\\Users\\haech\\Desktop\\insta_project_data\\gw_delete.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2376"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(map(str, list(np.unique(gw_delete['inner_id'])))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 광고성 단어가 포함된 id제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 지역_delete 지역 경로 설정\n",
    "# gw_delete_fin = pd.read_csv(r'C:\\Users\\haech\\Insta_profiling\\function\\gw_delete_over_1000.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1928"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(map(str, list(np.unique(gw_delete_fin['inner_id'])))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(r'C:\\Users\\haech\\Desktop\\insta_project_data\\gw_delete_over_1000.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit_words = ['문의', 'DM', '고객님', '완판', '이벤트', '콜라보', '납품', '광고', '구매', '매진', '할인', '홍보', '협찬', '수입', '행사', '출시', '런칭', '론칭', '입양', '한정기획', '원플원', '스마트스토어', '발매', '품절', '체험단', '운영중', '오픈시간', '티져영상', '예약', '공식쇼핑몰', '본점', '캠페인', '인스타친구', '사업', '협회', '재입고', '최대지원금', '최저가', '요금', '지급', '개시', '입점']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = []\n",
    "id_list = []\n",
    "word_cnt = 1 # 광고 단어 포함 횟수\n",
    "\n",
    "for i, content in enumerate(df['content']):\n",
    "    cnt = 0\n",
    "    for word in limit_words:\n",
    "        if word in str(content):\n",
    "            cnt += 1\n",
    "            if cnt == word_cnt:\n",
    "                id_list.append(df['inner_id'][i])\n",
    "                idx.append(i)\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "for user_id in id_list:\n",
    "    df = df[df['inner_id']!=user_id]\n",
    "df.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "제거된 포스트 개수: 1144, 제거된 유저 명수: 398\n"
     ]
    }
   ],
   "source": [
    "print('제거된 포스트 개수: {}, 제거된 유저 명수: {}'.format(len(idx), len(list(set(id_list)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1530"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df\n",
    "len(list(map(str, list(np.unique(df['inner_id'])))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.to_csv('gw_word.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('gw_word.csv', index_col =[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.to_csv('gw_word.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dfdf = pd.read_csv('gw_word.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
